{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f37795a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.239 ðŸš€ Python-3.10.0 torch-2.2.2+cu121 CPU (Intel Core(TM) i5-4460  3.20GHz)\n",
      "Setup complete âœ… (4 CPUs, 18.7 GB RAM, 48.3/251.0 GB disk)\n"
     ]
    }
   ],
   "source": [
    "#@title ## Setup\n",
    "# !py -m pip install ultralyticsplus\n",
    "import ultralytics\n",
    "ultralytics.checks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0cf17312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !py -m pip install roboflow\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import glob\n",
    "from getpass import getpass\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "from roboflow import Roboflow, load_roboflow_api_key\n",
    "from roboflow.core.workspace import Workspace, count_comparisons, check_box_size\n",
    "from roboflow.config import DEMO_KEYS\n",
    "from roboflow.adapters import rfapi\n",
    "\n",
    "class WorkspaceLocalhost(Workspace):\n",
    "    def active_learning(\n",
    "        self,\n",
    "        raw_data_location: str = \"\",\n",
    "        raw_data_extension: str = \"\",\n",
    "        inference_endpoint: list = [],\n",
    "        upload_destination: str = \"\",\n",
    "        conditionals: dict = {},\n",
    "        use_localhost: bool = False,\n",
    "    ) -> str:\n",
    "        \"\"\"perform inference on each image in directory and upload based on conditions\n",
    "        @params:\n",
    "            raw_data_location: (str) = folder of frames to be processed\n",
    "            raw_data_extension: (str) = extension of frames to be processed\n",
    "            inference_endpoint: (List[str, int]) = name of the project\n",
    "            upload_destination: (str) = name of the upload project\n",
    "            conditionals: (dict) = dictionary of upload conditions\n",
    "            use_localhost: (bool) = determines if local http format used or remote endpoint\n",
    "        \"\"\"  # noqa: E501 // docs\n",
    "        prediction_results = []\n",
    "\n",
    "        # ensure that all fields of conditionals have a key:value pair\n",
    "        conditionals[\"target_classes\"] = [] if \"target_classes\" not in conditionals else conditionals[\"target_classes\"]\n",
    "        conditionals[\"confidence_interval\"] = (\n",
    "            [30, 99] if \"confidence_interval\" not in conditionals else conditionals[\"confidence_interval\"]\n",
    "        )\n",
    "        conditionals[\"required_class_variance_count\"] = (\n",
    "            1 if \"required_class_variance_count\" not in conditionals else conditionals[\"required_class_variance_count\"]\n",
    "        )\n",
    "        conditionals[\"required_objects_count\"] = (\n",
    "            1 if \"required_objects_count\" not in conditionals else conditionals[\"required_objects_count\"]\n",
    "        )\n",
    "        conditionals[\"required_class_count\"] = (\n",
    "            0 if \"required_class_count\" not in conditionals else conditionals[\"required_class_count\"]\n",
    "        )\n",
    "        conditionals[\"minimum_size_requirement\"] = (\n",
    "            float(\"-inf\")\n",
    "            if \"minimum_size_requirement\" not in conditionals\n",
    "            else conditionals[\"minimum_size_requirement\"]\n",
    "        )\n",
    "        conditionals[\"maximum_size_requirement\"] = (\n",
    "            float(\"inf\") if \"maximum_size_requirement\" not in conditionals else conditionals[\"maximum_size_requirement\"]\n",
    "        )\n",
    "\n",
    "        # check if inference_model references endpoint or local\n",
    "        local = \"http://172.20.208.1:9001/\" if use_localhost else None\n",
    "\n",
    "#         inference_model = (\n",
    "#             self.project(inference_endpoint[0]).version(version_number=inference_endpoint[1], local=local).model\n",
    "#         )\n",
    "\n",
    "#         rf = Roboflow(api_key=\"tGo2415ulPekOjTMO3Ts\")\n",
    "#         project = rf.workspace(\"303-tywrd\").project(inference_endpoint[0])\n",
    "#         inference_model = project.version(version_number=inference_endpoint[1]).model\n",
    "\n",
    "        # infer on a local image\n",
    "#         print(model.predict(\"your_image.jpg\", confidence=40, overlap=30).json())\n",
    "\n",
    "        # hugging-face\n",
    "        from ultralyticsplus import YOLO, download_from_hub\n",
    "\n",
    "        hub_model_id = \"chanelcolgate/chamdiemgianhang-vsk-v5\" # @param {type:\"string\"}\n",
    "\n",
    "        # Load a model\n",
    "        model_path = download_from_hub(hub_model_id)\n",
    "        inference_model = YOLO(model_path)\n",
    "    \n",
    "    \n",
    "        upload_project = self.project(upload_destination)\n",
    "\n",
    "#         print(\"inference reference point: \", inference_model)\n",
    "        print(\"upload destination: \", upload_project)\n",
    "\n",
    "        # check if raw data type is cv2 frame\n",
    "        if issubclass(type(raw_data_location), np.ndarray):\n",
    "            globbed_files = [raw_data_location]\n",
    "        else:\n",
    "            globbed_files = glob.glob(raw_data_location + \"/*\" + raw_data_extension)\n",
    "\n",
    "        image1 = globbed_files[0]\n",
    "        similarity_timeout_counter = 0\n",
    "\n",
    "        for index, image in enumerate(globbed_files):\n",
    "            try:\n",
    "                print(\n",
    "                    \"*** Processing image [\" + str(index + 1) + \"/\" + str(len(globbed_files)) + \"] - \" + image + \" ***\"\n",
    "                )\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            if \"similarity_confidence_threshold\" in conditionals.keys():\n",
    "                image2 = image\n",
    "                # measure the similarity of two images using CLIP (hits an endpoint hosted by Roboflow)   # noqa: E501 // docs\n",
    "                similarity = clip_encode(image1, image2, CLIP_FEATURIZE_URL)\n",
    "                similarity_timeout_counter += 1\n",
    "\n",
    "                if (\n",
    "                    similarity <= conditionals[\"similarity_confidence_threshold\"]\n",
    "                    or similarity_timeout_counter == conditionals[\"similarity_timeout_limit\"]\n",
    "                ):\n",
    "                    image1 = image\n",
    "                    similarity_timeout_counter = 0\n",
    "                else:\n",
    "                    print(image2 + \" --> similarity too high to --> \" + image1)\n",
    "                    continue  # skip this image if too similar or counter hits limit\n",
    "\n",
    "            # predictions = inference_model.predict(image).json()[\"predictions\"]\n",
    "            import json\n",
    "            predictions = json.loads(inference_model.predict(image)[0].tojson())\n",
    "#             print(predictions)\n",
    "            for prediction in predictions:\n",
    "                prediction.update({\n",
    "                    \"class_id\": int(prediction[\"class\"]),\n",
    "                    \"class\": prediction[\"name\"]}\n",
    "                )\n",
    "                \n",
    "            # collect all predictions to return to user at end\n",
    "            prediction_results.append({\"image\": image, \"predictions\": predictions})\n",
    "\n",
    "            # compare object and class count of predictions if enabled,\n",
    "            # continue if not enough occurrences\n",
    "            if not count_comparisons(\n",
    "                predictions,\n",
    "                conditionals[\"required_objects_count\"],\n",
    "                conditionals[\"required_class_count\"],\n",
    "                conditionals[\"target_classes\"],\n",
    "            ):\n",
    "                print(\" [X] image failed count cases\")\n",
    "                continue\n",
    "\n",
    "            # iterate through all predictions\n",
    "            for i, prediction in enumerate(predictions):\n",
    "                print(i)\n",
    "\n",
    "                # check if box size of detection fits requirements\n",
    "#                 if not check_box_size(\n",
    "#                     prediction,\n",
    "#                     conditionals[\"minimum_size_requirement\"],\n",
    "#                     conditionals[\"maximum_size_requirement\"],\n",
    "#                 ):\n",
    "#                     print(\" [X] prediction failed box size cases\")\n",
    "#                     continue\n",
    "\n",
    "                # compare confidence of detected object to confidence thresholds\n",
    "                # confidence comes in as a .XXX instead of XXX%\n",
    "                if (\n",
    "                    prediction[\"confidence\"] * 100 >= conditionals[\"confidence_interval\"][0]\n",
    "                    and prediction[\"confidence\"] * 100 <= conditionals[\"confidence_interval\"][1]\n",
    "                ):\n",
    "                    # filter out non-target_class uploads if enabled\n",
    "                    if (\n",
    "                        len(conditionals[\"target_classes\"]) > 0\n",
    "                        and prediction[\"class\"] not in conditionals[\"target_classes\"]\n",
    "                    ):\n",
    "                        print(\" [X] prediction failed target_classes\")\n",
    "                        continue\n",
    "\n",
    "                    # upload on success!\n",
    "                    print(\" >> image uploaded!\")\n",
    "                    upload_project.upload(image, num_retry_uploads=3)\n",
    "                    import shutil\n",
    "                    shutil.move(image, image.replace('dataset', 'unlabeled28'))\n",
    "                    break\n",
    "\n",
    "        # return predictions with filenames if globbed images from dir,\n",
    "        # otherwise return latest prediction result\n",
    "        return prediction_results if type(raw_data_location) is not ndarray else prediction_results[-1][\"predictions\"]\n",
    "    \n",
    "class RoboflowLocalhost(Roboflow):\n",
    "    def workspace(self, the_workspace=None):\n",
    "        sys.stdout.write(\"\\r\" + \"loading Roboflow workspace...\")\n",
    "        sys.stdout.write(\"\\n\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        if the_workspace is None:\n",
    "            the_workspace = self.current_workspace\n",
    "\n",
    "        if self.api_key in DEMO_KEYS:\n",
    "            return Workspace({}, self.api_key, the_workspace, self.model_format)\n",
    "        workspace_api_key = load_roboflow_api_key(the_workspace)\n",
    "        api_key = workspace_api_key or self.api_key\n",
    "\n",
    "        list_projects = rfapi.get_workspace(api_key, the_workspace)\n",
    "        return WorkspaceLocalhost(list_projects, api_key, the_workspace, self.model_format)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f628632b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset 1\n",
    "import os\n",
    "from roboflow import Roboflow\n",
    "\n",
    "rf = RoboflowLocalhost(api_key=\"tGo2415ulPekOjTMO3Ts\")\n",
    "workspace = rf.workspace(\"303-tywrd\")\n",
    "\n",
    "raw_data_location = './dataset/'\n",
    "raw_data_extension = \".jpg\"\n",
    "\n",
    "inference_endpoint = [\"vsk-1\", 1]\n",
    "upload_destination = \"vsk-1\"\n",
    "\n",
    "# conditionals = {\n",
    "#     # \"similarity_confidence_threshold\": 0\n",
    "#     \"confidence_interval\": [10, 90],\n",
    "#     \"target_classes\": [\"BOM_GEN\", \"BOM_JUN\", \"BOM_KID\", \"BOM_SAC\", \"BOM_VTG\", \"BOM_YTV\",\n",
    "#                        \"TUI_GEN\", \"TUI_JUN\", \"TUI_KID\", \"TUI_SAC\", \"TUI_THV\", \"TUI_THX\",\n",
    "#                        \"TUI_VTG\", \"TUI_YTV\"],\n",
    "# }\n",
    "\n",
    "conditionals = {\n",
    "    # \"similarity_confidence_threshold\": 0\n",
    "    \"confidence_interval\": [10, 90],\n",
    "    \"target_classes\": [\"BOM_GEN\", \"TUI_GEN\"],\n",
    "}\n",
    "\n",
    "workspace.active_learning(\n",
    "    raw_data_location=raw_data_location,\n",
    "    raw_data_extension=raw_data_extension,\n",
    "    inference_endpoint=inference_endpoint,\n",
    "    upload_destination=upload_destination,\n",
    "    conditionals=conditionals,\n",
    "    use_localhost=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23b6081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset 2\n",
    "import os\n",
    "from roboflow import Roboflow\n",
    "\n",
    "rf = RoboflowLocalhost(api_key=\"tGo2415ulPekOjTMO3Ts\")\n",
    "workspace = rf.workspace(\"303-tywrd\")\n",
    "\n",
    "raw_data_location = './dataset/'\n",
    "raw_data_extension = \".jpg\"\n",
    "\n",
    "inference_endpoint = [\"vsk-1\", 1]\n",
    "upload_destination = \"vsk-1\"\n",
    "\n",
    "conditionals = {\n",
    "    # \"similarity_confidence_threshold\": 0\n",
    "    \"confidence_interval\": [10, 90],\n",
    "    \"target_classes\": [\"BOM_GEN\", \"BOM_JUN\", \"BOM_KID\", \"BOM_SAC\", \"BOM_VTG\", \"BOM_YTV\",\n",
    "                       \"TUI_GEN\", \"TUI_JUN\", \"TUI_KID\", \"TUI_SAC\", \"TUI_THV\", \"TUI_THX\",\n",
    "                       \"TUI_VTG\", \"TUI_YTV\"],\n",
    "}\n",
    "\n",
    "workspace.active_learning(\n",
    "    raw_data_location=raw_data_location,\n",
    "    raw_data_extension=raw_data_extension,\n",
    "    inference_endpoint=inference_endpoint,\n",
    "    upload_destination=upload_destination,\n",
    "    conditionals=conditionals,\n",
    "    use_localhost=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
